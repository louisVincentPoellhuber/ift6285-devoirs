{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisation\n",
    "\n",
    "## 0.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import chardet\n",
    "import codecs\n",
    "import jellyfish\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\data\\hw2\"\n",
    "#DATA_PATH = r\"C:\\Users\\barka\\Desktop\\NLP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(file_path):\n",
    "\n",
    "    with open(file_path, 'rb') as file:\n",
    "        rawdata = file.read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    return result['encoding']\n",
    "\n",
    "def get_word_counter(file_path, encoding):\n",
    "    word_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r', encoding=encoding) as file:\n",
    "        for line in file:\n",
    "            \n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:\n",
    "                word = parts[1]\n",
    "                \n",
    "                word_counter[word.strip()] += 1\n",
    "\n",
    "    # Return the Counter containing word frequencies\n",
    "    return word_counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Check data quality\n",
    "Make sure Python is reading the lines correctly. And it does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 rows in the file. There should be 1000. Correct number? True\n",
      "Encoding : utf-8\n",
      "There are 201315 rows in the file. There should be 201 315. Correct number? True\n"
     ]
    }
   ],
   "source": [
    "# Check ypos\n",
    "typos_file = open(DATA_PATH + r\"\\typo-0.2.txt\")\n",
    "\n",
    "for i, row in enumerate(typos_file):\n",
    "    #print(row)\n",
    "    pass\n",
    "\n",
    "print(f\"There are {i+1} rows in the file. There should be 1000. Correct number? {i+1==1000}\")\n",
    "\n",
    "# Check vocabulary\n",
    "\n",
    "encoding = detect_encoding(DATA_PATH + r\"\\voc-1bwc.txt\") #check encoding\n",
    "print(f\"Encoding : {encoding}\")\n",
    "\n",
    "voc = open(DATA_PATH + r\"\\voc-1bwc.txt\", encoding=encoding)\n",
    "\n",
    "for i, row in enumerate(voc):\n",
    "    #print(row)\n",
    "    pass\n",
    "\n",
    "print(f\"There are {i+1} rows in the file. There should be 201 315. Correct number? {i+1==201315}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201315\n"
     ]
    }
   ],
   "source": [
    "vocab = get_word_counter(DATA_PATH + r\"\\voc-1bwc.txt\", encoding)\n",
    "print(len(vocab)) # just to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing various distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Distancc functions\n",
    "\n",
    "### 1.1.a - Edit distance \n",
    "Taken from the blog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word, N=sum(vocab.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return vocab[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in vocab)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.b -  Jaro distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_correction(word):\n",
    "    max_distance = float('-inf')\n",
    "    max_word = \"\"\n",
    "    \n",
    "    for w in vocab :\n",
    "        \n",
    "        distance = jellyfish.jaro_similarity(word, w)\n",
    "        \n",
    "        if max_distance < distance :\n",
    "            \n",
    "            max_distance = distance\n",
    "            max_word = w\n",
    "    \n",
    "    return max_word, max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.c -  Jaro-Winkler distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_winkler_correction(word):\n",
    "    max_distance = float('-inf')\n",
    "    max_word = \"\"\n",
    "    \n",
    "    for w in vocab :\n",
    "        \n",
    "        distance = jellyfish.jaro_winkler_similarity(word, w)\n",
    "        \n",
    "        if max_distance < distance :\n",
    "            \n",
    "            max_distance = distance\n",
    "            max_word = w\n",
    "    \n",
    "    return max_word, max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Testing the correction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction('speling') #Edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sperling', 0.9583333333333334)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaro_correction('speling') #Jaro distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('spelling', 0.975)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaro_winkler_correction('speling') #Jaro-Winkler distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Running the correction methods on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 - Get the typos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH + r\"\\typo-0.2.txt\", \"r\", encoding=encoding) as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Regex\n",
    "typo_pattern = r'<typo orig=\"([^\"]+)\">([^<]+)</typo>'\n",
    "typos = re.findall(typo_pattern, text)\n",
    "typos = dict(typos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 - Apply correction methods: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Correcting Typos:  18%|█▊        | 295/1601 [00:38<02:21,  9.20it/s]"
     ]
    }
   ],
   "source": [
    "corrected_typos = dict()\n",
    "\n",
    "for orig, typo in tqdm(typos.items(), desc=\"Correcting Typos\"):\n",
    "    correction = jaro_winkler_correction(typo)[0]  # Jaro-Winkler distance\n",
    "    corrected_typos[orig] = correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of corrected typos : 1601\n",
      " Number of typos : 1601\n"
     ]
    }
   ],
   "source": [
    "print(f\" Number of corrected typos : {len(corrected_typos)}\")\n",
    "print(f\" Number of typos : {len(typos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the text file with the corrections : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (if u want and u have time u can write otherwise i will do it later at night)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
