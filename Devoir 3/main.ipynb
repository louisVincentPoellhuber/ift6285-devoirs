{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Python\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import utils\n",
    "\n",
    "# Global variables\n",
    "DATA_PATH = os.getcwd().split(\"Devoir 3\")[0] + \"data\\\\\"\n",
    "MODEL_PATH = os.getcwd().split(\"Devoir 3\")[0] + \"models\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Get the data\n",
    "Test with 1BWC short. I think we should read the core tutorials on [Gensim's documentation website](https://radimrehurek.com/gensim/auto_examples/index.html#documentation). \n",
    "\n",
    "Here's my opinion on how we could separate corpus / documents: \n",
    "1. The 10 different slices each represent a document, the corpus being the 10 slices. \n",
    "2. Concatenate all 10 slices, make that our corpus. Each sentence is a document. \n",
    "3. Same as 2., but we make sentence slices (10 sentences, 20 sentences, 100 sentences, etc.) as the documents. \n",
    "\n",
    "Gensim is smart, it allows us to load corpora into memory as lists, but also in [streaming](https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial). We'll create our own class, as described in the tutorials. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Test class for a singular corpus, I.E. a corpus that fits within a single file. \n",
    "Parameters\n",
    "    data_path: The path to look for the data.\n",
    "\n",
    "Returns a generator object to iterate over all the documents in the corpus once. \n",
    "'''\n",
    "class SingularCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath(self.data_path)\n",
    "        for line in open(corpus_path, encoding=\"utf8\"):\n",
    "            # one document per line\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "'''Test class for a sliced corpus, I.E. a corpus that is made up of multiple files. \n",
    "Parameters\n",
    "    folder_path: The path to look for the folder containing the slices. It assumes that \n",
    "                 the folder contains only slices for this specific corpus. \n",
    "\n",
    "Returns a generator object to iterate over all the documents in the corpus once. \n",
    "'''\n",
    "class SlicedCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "\n",
    "    def __iter__(self):\n",
    "        for data_path in os.listdir(self.folder_path):\n",
    "            corpus_path = datapath(self.folder_path + data_path)\n",
    "            print(corpus_path)\n",
    "            for line in open(corpus_path, encoding=\"utf8\"):\n",
    "                # one document per line\n",
    "                yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Framework architecture\n",
    "In this section I'll work on developing a structure that makes it easy for us to test, train, save and evaluate models.\n",
    "\n",
    "### Test with Slice 0 of 1BWC Short\n",
    "\n",
    "**Training & Saving**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 95.27s.\n"
     ]
    }
   ],
   "source": [
    "slice0 = SingularCorpus(DATA_PATH + \"1bshort\\\\news.en-00000-of-00100.txt\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "model = gensim.models.Word2Vec(sentences=slice0)\n",
    "print(f\"Training took {round(time.perf_counter() - tic, 2)}s.\")\n",
    "model.save(MODEL_PATH + \"1bs-0-w2v.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading, Word Vectors and Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37116"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load(MODEL_PATH + \"1bs-0-w2v.model\")\n",
    "wv = model.wv\n",
    "vocab = wv.index_to_key\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with all slices from 1BWC short\n",
    "**Training & Saving**\n",
    "Since it took 95s to train on a single slice, it should take about 16 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00000-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00001-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00002-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00003-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00004-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00005-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00006-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00007-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00008-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00009-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00000-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00001-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00002-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00003-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00004-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00005-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00006-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00007-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00008-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00009-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00000-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00001-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00002-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00003-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00004-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00005-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00006-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00007-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00008-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00009-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00000-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00001-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00002-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00003-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00004-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00005-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00006-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00007-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00008-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00009-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00000-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00001-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00002-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00003-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00004-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00005-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00006-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00007-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00008-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00009-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00000-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00001-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00002-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00003-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00004-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00005-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00006-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00007-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00008-of-00100.txt\n",
      "c:\\Users\\Louis\\Documents\\University\\Masters\\A23\\NLP\\Devoirs\\ift6285-devoirs\\Data\\1bshort\\news.en-00009-of-00100.txt\n",
      "Training took 749.92s.\n"
     ]
    }
   ],
   "source": [
    "onebwcshort = SlicedCorpus(DATA_PATH + \"1bshort\\\\\")\n",
    "\n",
    "tic = time.perf_counter()\n",
    "model = gensim.models.Word2Vec(sentences=onebwcshort)\n",
    "print(f\"Training took {round(time.perf_counter() - tic, 2)}s.\")\n",
    "model.save(MODEL_PATH + \"1bs-all-w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107374"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec.load(MODEL_PATH + \"1bs-all-w2v.model\")\n",
    "wv = model.wv\n",
    "vocab = wv.index_to_key\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Réponse aux questions\n",
    "C'est le temps de répondre aux questions!\n",
    "\n",
    "## 2.1 Entraînement de plusieurs modèles\n",
    "- Essayer plusieurs familles d'embeddings?\n",
    "- Essayer plusieurs tailles de corpus\n",
    "- Essayer plusieurs tailles de documents\n",
    "- Essayer plusieurs tailles de vecteurs\n",
    "\n",
    "Tout ça dans le but d'optimiser les performances du benchmark du point 2.2.\n",
    "\n",
    "***Bonne pratique***: J'ai créé un dossier pour conserver nos modèles, `MODEL_PATH`. On devrait effacer les modèles qui ne sont pas finaux. Ceci va être pratique pour lorsqu'on va vouloir évaluer les modèles en masse, parce qu'on pourra tout simplement lister les modèles contenus dans le folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from devoir3 import train_w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Benchmark sur TOEFL\n",
    "J'ai mis le contenu du fichier tar qu'il nous a envoyé dans le `data` path dans le root folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Voisins\n",
    "Appliqué pour un seul modèle. J'imagine qu'on serait mieux d'utiliser notre meilleur modèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = MODEL_PATH + \"our_model.model\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
